{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import torch\n",
    "from nnAudio import features as spectrograms\n",
    "import io\n",
    "import torchaudio\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Clear memory cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "train_data_path = 'data/raw/'\n",
    "processed_data_path = 'data/processed_data/'\n",
    "\n",
    "def unzip_to_buffer(zip_ref, track):\n",
    "    \"\"\"Open the zipfolder and save file-like object to memory.\"\"\"\n",
    "    waveform_binary = zip_ref.read(track)\n",
    "    file_like_object = io.BytesIO(waveform_binary)\n",
    "\n",
    "    return file_like_object\n",
    "\n",
    "def generate_spectrograms(waveforms):\n",
    "    \"\"\"Generate spectrograms from a batch of waveforms.\n",
    "    \n",
    "    Args:\n",
    "        waveforms: numpy array of shape (batch_size, samples) or torch tensor\n",
    "    Returns:\n",
    "        batch of spectrograms of shape (batch_size, freq_bins, time_frames)\n",
    "    \"\"\"\n",
    "    # If input is a single waveform, add batch dimension\n",
    "    if len(waveforms.shape) == 1:\n",
    "        waveforms = waveforms[None, :]\n",
    "        \n",
    "    # Convert to tensor if not already\n",
    "    if not torch.is_tensor(waveforms):\n",
    "        waveforms = torch.tensor(waveforms)\n",
    "    \n",
    "    waveforms = waveforms.to(device)  # Move batch to device\n",
    "    spec_converter = spectrograms.STFT(n_fft=2048).to(device)\n",
    "    spectrograms_batch = spec_converter(waveforms)\n",
    "    spectrograms_batch = spectrograms_batch.cpu().detach().numpy()\n",
    "    spectrograms_db = librosa.amplitude_to_db(np.abs(spectrograms_batch), ref=np.max)\n",
    "    \n",
    "    return spectrograms_db[:, :, :, 0]  # Return all spectrograms, keeping batch dimension\n",
    "\n",
    "def save_track_data(track_name, spectrograms, path, category, is_training):\n",
    "    \"\"\"Save track data to disk.\"\"\"\n",
    "    if is_training:\n",
    "        save_dir = os.path.join(path, 'train', category)\n",
    "    else:\n",
    "        save_dir = os.path.join(path, 'test', category)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    for segment_idx, spectrogram in enumerate(spectrograms):\n",
    "        audio_file_row = pd.DataFrame([{\n",
    "            'file_name': f'{track_name}_segment_{segment_idx}',\n",
    "            'spectrogram': spectrogram\n",
    "        }])\n",
    "        audio_file_row.to_pickle(os.path.join(save_dir, f'{track_name}_segment_{segment_idx}.pkl'))\n",
    "\n",
    "def is_training_track(track_components):\n",
    "    \"\"\"Check if track is valid for processing.\"\"\"\n",
    "    if track_components[0] == \"train\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def create_lossless_waveform(wav_buffer):\n",
    "    \"\"\"Create lossless waveform from wav buffer.\"\"\"\n",
    "    wav_buffer.seek(0)\n",
    "    waveform = librosa.load(wav_buffer, sr=None)\n",
    "    return waveform\n",
    "\n",
    "def create_lossy_waveform(wav_buffer):\n",
    "    \"\"\"Create lossy waveform from wav buffer.\"\"\"\n",
    "    wav_buffer.seek(0)\n",
    "    mp3_buffer = io.BytesIO()\n",
    "    AudioSegment.from_wav(wav_buffer).export(mp3_buffer, format=\"mp3\", bitrate=\"320k\")\n",
    "    mp3_buffer.seek(0)\n",
    "    waveform = librosa.load(mp3_buffer, sr=None)\n",
    "    return waveform\n",
    "\n",
    "def split_waveform(waveform, sr, segment_duration=6):\n",
    "    \"\"\"Split waveform into segments of specified duration.\n",
    "    Returns numpy array of shape (num_segments, samples_per_segment)\"\"\"\n",
    "    samples_per_segment = sr * segment_duration\n",
    "    num_segments = int(np.ceil(len(waveform) / samples_per_segment))\n",
    "    segments = np.zeros((num_segments, samples_per_segment), dtype=np.float32)\n",
    "    print('\\rnum_segments   | ', num_segments, end='', flush=True)\n",
    "    for i in range(num_segments):\n",
    "        start = i * samples_per_segment\n",
    "        end = min(start + samples_per_segment, len(waveform))\n",
    "        segment = waveform[start:end]\n",
    "        \n",
    "        # Pad last segment if needed\n",
    "        if len(segment) < samples_per_segment:\n",
    "            padding = np.zeros(samples_per_segment - len(segment), dtype=np.float32)\n",
    "            segment = np.concatenate([segment, padding])\n",
    "            \n",
    "        segments[i] = segment\n",
    "    \n",
    "    return segments\n",
    "\n",
    "def process_track(zip_ref, track, track_name, path, is_training):\n",
    "    \"\"\"Process a single track to generate and save spectrograms.\"\"\"\n",
    "    wav_buffer = unzip_to_buffer(zip_ref, track)\n",
    "    \n",
    "    lossless_waveform, lossless_sr = create_lossless_waveform(wav_buffer)\n",
    "    lossy_waveform, lossy_sr = create_lossy_waveform(wav_buffer)\n",
    "    \n",
    "    # Split waveforms into 30-second segments\n",
    "    lossless_segments = split_waveform(lossless_waveform, lossless_sr)\n",
    "    lossy_segments = split_waveform(lossy_waveform, lossy_sr)\n",
    "\n",
    "    lossless_spectrograms = generate_spectrograms(lossless_segments)\n",
    "    lossy_spectrograms = generate_spectrograms(lossy_segments)\n",
    "\n",
    "    save_track_data(track_name, lossless_spectrograms, path, 'lossless', is_training)\n",
    "    save_track_data(track_name, lossy_spectrograms, path, 'lossy', is_training)\n",
    "\n",
    "def is_suitable_file(components):\n",
    "    if components[-1] == 'mixture.wav':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def extract_musdb_mixture_files(raw_data_path, processed_data_path):\n",
    "    \"\"\"Extract and process audio files from MUSDB dataset.\"\"\"\n",
    "    counter = 0\n",
    "    print('extracting files from zip', raw_data_path + 'musdb18hq.zip')\n",
    "    with zipfile.ZipFile(raw_data_path + 'musdb18hq.zip', 'r') as zip_ref:\n",
    "        for track in zip_ref.namelist():\n",
    "            components = track.split('/')\n",
    "                \n",
    "            if is_suitable_file(components):\n",
    "                is_training = is_training_track(components)\n",
    "                track_name = components[1]\n",
    "                print('Is training = ', is_training, '|  trackname = ', track_name)\n",
    "                \n",
    "                process_track(zip_ref, track, track_name, processed_data_path, is_training)\n",
    "            \n",
    "                counter += 1\n",
    "                print(counter, 'files processed')\n",
    "                print('----------------------------------------')\n",
    "    print('All audio files processed. Saving to disk')\n",
    "    \n",
    "extract_musdb_mixture_files(train_data_path, processed_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "train_data_path = 'data/processed_data/train'\n",
    "test_data_path = 'data/processed_data/test'\n",
    "\n",
    "# Custom dataset class for spectrograms\n",
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, path, target_size=(512, 512)):\n",
    "        self.path = path\n",
    "        self.samples = []\n",
    "        self.target_size = target_size\n",
    "\n",
    "        # Load all spectrogram files and their labels\n",
    "        for catergory in ['lossy', 'lossless']:\n",
    "            catergory_path = os.path.join(path, catergory)\n",
    "            if not os.path.exists(catergory_path):\n",
    "                raise RuntimeError(f\"Directory not found: {catergory_path}\")\n",
    "            for file in os.listdir(catergory_path):\n",
    "                if file.endswith('.pkl'):\n",
    "                    self.samples.append({\n",
    "                        'path': os.path.join(catergory_path, file),\n",
    "                        'label': 0 if catergory == 'lossless' else 1\n",
    "                    })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def resize_spectrogram(self, spec):\n",
    "        # print('\\rresizing spectrogram from ', spec.shape, 'to ', self.target_size, end='', flush=True)\n",
    "        \n",
    "        # Add batch and channel dimensions before resizing\n",
    "        spec = spec.unsqueeze(0).unsqueeze(0)  # Shape becomes (1, 1, H, W)\n",
    "        \n",
    "        resize_transform = transforms.Compose([\n",
    "            transforms.Resize(self.target_size)\n",
    "        ])\n",
    "        \n",
    "        spec = resize_transform(spec)\n",
    "        spec = spec.squeeze(0).squeeze(0)  # Remove the batch and channel dimensions\n",
    "        return spec\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            sample = self.samples[idx]\n",
    "            df = pd.read_pickle(sample['path'])\n",
    "            \n",
    "            spectrogram = df['spectrogram'].iloc[0]\n",
    "            # print(f\"\\nLoading sample {idx}\")\n",
    "            # print(f\"Raw spectrogram shape: {spectrogram.shape}\")\n",
    "            \n",
    "            # Normalize spectrogram\n",
    "            spectrogram = (spectrogram - spectrogram.mean()) / (spectrogram.std() + 1e-6)\n",
    "            \n",
    "            # Convert to tensor and pad/crop\n",
    "            spectrogram = torch.FloatTensor(spectrogram)\n",
    "            spectrogram = self.resize_spectrogram(spectrogram)\n",
    "            spectrogram = spectrogram.unsqueeze(0)  # Add channel dimension\n",
    "            # print(f\"Final tensor shape (with channel dimension): {spectrogram.shape}\")\n",
    "            \n",
    "            return spectrogram, sample['label']\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample {idx}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Define the CNN model\n",
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "            \n",
    "            # Third conv block\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "            \n",
    "            # Fourth conv block\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "            \n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    \n",
    "# Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AudioClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = SpectrogramDataset(train_data_path)\n",
    "test_dataset = SpectrogramDataset(test_data_path)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=16,  # If memory-constrained\n",
    "    shuffle=True,\n",
    "    # num_workers = 1,  # Get number of CPU cores\n",
    "    pin_memory=True,  # Faster GPU transfer\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,  \n",
    "    shuffle=False,\n",
    "    # num_workers = multiprocessing.cpu_count(),\n",
    "    pin_memory=True,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "max_epochs = 100  # Set a maximum number of epochs\n",
    "patience = 3  # Number of epochs to wait for improvement\n",
    "best_accuracy = 0\n",
    "epochs_without_improvement = 0\n",
    "epoch = 0\n",
    "\n",
    "while epochs_without_improvement < patience and epoch < max_epochs:\n",
    "    print('epoch', epoch)\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, (spectrograms, labels) in enumerate(train_loader):\n",
    "        print(f'\\rProcessing batch {i+1}/{len(train_loader)}', end='', flush=True)\n",
    "        \n",
    "        # Clear memory cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "         \n",
    "        spectrograms, labels = spectrograms.to(device, non_blocking=True), labels.to(device, non_blocking=True) # non_blocking=True is allows the data to be loaded in parallel\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True) # uses None instead of 0 to reduce memory usage\n",
    "        outputs = model(spectrograms)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "                \n",
    "        # Explicitly clear some variables\n",
    "        del outputs\n",
    "        del loss\n",
    "    \n",
    "    print('')  # New line after batch processing is complete\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Epoch [{epoch+1}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    # Check if accuracy improved\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        \n",
    "    print(f'Best accuracy so far: {best_accuracy:.2f}%')\n",
    "    print(f'Epochs without improvement: {epochs_without_improvement}')\n",
    "\n",
    "    # Create models directory if it doesn't exist\n",
    "    import os\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "\n",
    "    # Save the model checkpoint\n",
    "    checkpoint_path = f'models/model_checkpoint_epoch_{epoch+1}.pth'\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': epoch_loss,\n",
    "        'accuracy': accuracy\n",
    "    }, checkpoint_path)\n",
    "    print(f'Checkpoint saved to {checkpoint_path}')\n",
    "    \n",
    "    epoch += 1\n",
    "\n",
    "print(f'\\nTraining stopped after {epoch} epochs')\n",
    "if epochs_without_improvement >= patience:\n",
    "    print(f'Early stopping triggered: no improvement for {patience} consecutive epochs')\n",
    "elif epoch >= max_epochs:\n",
    "    print('Maximum number of epochs reached')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the latest checkpoint\n",
    "checkpoint_files = [f for f in os.listdir('models') if f.startswith('model_checkpoint_epoch_')]\n",
    "if checkpoint_files:\n",
    "    # Get the latest checkpoint file\n",
    "    latest_checkpoint = max(checkpoint_files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "    checkpoint_path = os.path.join('models', latest_checkpoint)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    print(f'Loaded checkpoint from {checkpoint_path}')\n",
    "    print(f'Epoch: {start_epoch}, Loss: {checkpoint[\"loss\"]:.4f}, Accuracy: {checkpoint[\"accuracy\"]:.2f}%')\n",
    "else:\n",
    "    print('No checkpoint found. Starting from scratch.')\n",
    "\n",
    "# Evaluate the model on test data\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0.0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "print(\"\\nEvaluating model on test data...\")\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    for spectrograms, labels in test_loader:\n",
    "        # Move data to device\n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(spectrograms)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # Get predictions\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Store predictions and labels\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Clean up memory\n",
    "        del outputs\n",
    "        del loss\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "test_accuracy = accuracy_score(all_labels, all_predictions) * 100\n",
    "test_recall = recall_score(all_labels, all_predictions, average='macro') * 100\n",
    "test_precision = precision_score(all_labels, all_predictions, average='macro') * 100\n",
    "test_f1 = f1_score(all_labels, all_predictions, average='macro') * 100\n",
    "test_loss = test_loss / len(test_loader)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "print(f'Test Recall: {test_recall:.2f}%')\n",
    "print(f'Test Precision: {test_precision:.2f}%')\n",
    "print(f'Test F1 Score: {test_f1:.2f}%')\n",
    "\n",
    "# Set model back to training mode\n",
    "model.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Run the API server\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 53\u001b[0m     \u001b[43muvicorn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.0.0.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kasey\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\uvicorn\\main.py:579\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(app, host, port, uds, fd, loop, http, ws, ws_max_size, ws_max_queue, ws_ping_interval, ws_ping_timeout, ws_per_message_deflate, lifespan, interface, reload, reload_dirs, reload_includes, reload_excludes, reload_delay, workers, env_file, log_config, log_level, access_log, proxy_headers, server_header, date_header, forwarded_allow_ips, root_path, limit_concurrency, backlog, limit_max_requests, timeout_keep_alive, timeout_graceful_shutdown, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_version, ssl_cert_reqs, ssl_ca_certs, ssl_ciphers, headers, use_colors, app_dir, factory, h11_max_incomplete_event_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m         Multiprocess(config, target\u001b[38;5;241m=\u001b[39mserver\u001b[38;5;241m.\u001b[39mrun, sockets\u001b[38;5;241m=\u001b[39m[sock])\u001b[38;5;241m.\u001b[39mrun()\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 579\u001b[0m         \u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# pragma: full coverage\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kasey\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\uvicorn\\server.py:65\u001b[0m, in \u001b[0;36mServer.run\u001b[1;34m(self, sockets)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, sockets: \u001b[38;5;28mlist\u001b[39m[socket\u001b[38;5;241m.\u001b[39msocket] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msetup_event_loop()\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43msockets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msockets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kasey\\anaconda3\\envs\\pytorch\\Lib\\asyncio\\runners.py:186\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "# Create a FastAPI endpoint for model inference\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "import base64\n",
    "import io\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from neural_networks.cnn import AudioClassifier\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load the trained model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AudioClassifier()\n",
    "model.load_state_dict(torch.load('models/audio_classifier.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "class SpectrogramData(BaseModel):\n",
    "    spectrogram: str  # Base64 encoded spectrogram image\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(data: SpectrogramData):\n",
    "    try:\n",
    "        # Decode base64 string to image\n",
    "        spectrogram_bytes = base64.b64decode(data.spectrogram)\n",
    "        spectrogram_img = Image.open(io.BytesIO(spectrogram_bytes))\n",
    "        \n",
    "        # Convert to numpy array and preprocess\n",
    "        spectrogram_array = np.array(spectrogram_img)\n",
    "        \n",
    "        # Convert to tensor and add batch & channel dimensions\n",
    "        spectrogram_tensor = torch.FloatTensor(spectrogram_array).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Move to device\n",
    "        spectrogram_tensor = spectrogram_tensor.to(device)\n",
    "        \n",
    "        # Get prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(spectrogram_tensor)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            prediction = predicted.item()\n",
    "        \n",
    "        # Convert prediction to label\n",
    "        label = \"abnormal\" if prediction == 1 else \"normal\"\n",
    "        \n",
    "        return {\n",
    "            \"prediction\": label,\n",
    "            \"confidence\": float(torch.softmax(output, dim=1)[0][int(prediction)].item())\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Run the API server\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all 22 samples from the dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate rows and columns for subplot grid\n",
    "n_cols = 5\n",
    "n_rows = (22 + n_cols - 1) // n_cols  # Ceil division to fit all 22 samples\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "# Create new DataLoaders with batch_size=1 specifically for plotting\n",
    "plot_train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "plot_test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Plot all samples\n",
    "sample_count = 0\n",
    "for dataloader, title in [(plot_train_loader, \"Train\"), (plot_test_loader, \"Test\")]:\n",
    "    for spectrograms, _ in dataloader:\n",
    "        plt.subplot(n_rows, n_cols, sample_count + 1)\n",
    "        plt.imshow(spectrograms[0][0].cpu().numpy(), aspect='auto', origin='lower')\n",
    "        plt.title(f'{title} Sample {sample_count + 1}')\n",
    "        plt.colorbar()\n",
    "        sample_count += 1\n",
    "        if sample_count >= 22:  # Stop after 22 samples\n",
    "            break\n",
    "    if sample_count >= 22:\n",
    "        break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the total number of samples available\n",
    "print(f\"Total training samples: {len(train_dataset)}\")\n",
    "print(f\"Total test samples: {len(test_dataset)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
